{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Slidin' Videos: Use high-precision text tracking and semantic segmentation for chapters generation\n\n#### Please register for the Slidin' Videos challenge to get download URLs used in this notebook \n\n## 1. Deeplab finetuning\n#### DeepLabV3 model is a pretrained model for semantic segmentation of images. We will finetune it on Slidin' Videos dataset of slide titles\n\n# !git clone https://github.com/msminhas93/DeepLabv3FineTuning.git\n!pip install -q torch torchvision pandas scikit-learn\n!pip install -q opencv-python-headless Shapely Pillow easyocr\n","metadata":{"executionInfo":{"elapsed":19359,"status":"ok","timestamp":1685803538963,"user":{"displayName":"Ameck Dosseh","userId":"09603352587859208137"},"user_tz":-60},"id":"rp6SHN_OuD_Q","outputId":"a9f81267-9c1e-4540-8988-b997439ce830","execution":{"iopub.status.busy":"2023-06-11T00:36:30.520462Z","iopub.execute_input":"2023-06-11T00:36:30.520828Z","iopub.status.idle":"2023-06-11T00:36:52.286091Z","shell.execute_reply.started":"2023-06-11T00:36:30.520797Z","shell.execute_reply":"2023-06-11T00:36:52.284925Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"# !pip install -q Pillow==6.2.2","metadata":{"execution":{"iopub.status.busy":"2023-06-11T00:36:52.289498Z","iopub.execute_input":"2023-06-11T00:36:52.290175Z","iopub.status.idle":"2023-06-11T00:36:52.294969Z","shell.execute_reply.started":"2023-06-11T00:36:52.290119Z","shell.execute_reply":"2023-06-11T00:36:52.293732Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# from IPython.core.display import HTML\n# HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")","metadata":{"execution":{"iopub.status.busy":"2023-06-11T00:36:52.296240Z","iopub.execute_input":"2023-06-11T00:36:52.296751Z","iopub.status.idle":"2023-06-11T00:36:52.309861Z","shell.execute_reply.started":"2023-06-11T00:36:52.296713Z","shell.execute_reply":"2023-06-11T00:36:52.308938Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"!cp -r /kaggle/input/getdata/* /kaggle/working/","metadata":{"execution":{"iopub.status.busy":"2023-06-11T00:36:52.312645Z","iopub.execute_input":"2023-06-11T00:36:52.313142Z","iopub.status.idle":"2023-06-11T00:37:02.877351Z","shell.execute_reply.started":"2023-06-11T00:36:52.313100Z","shell.execute_reply":"2023-06-11T00:37:02.876065Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"!ls","metadata":{"executionInfo":{"elapsed":410,"status":"ok","timestamp":1685803609379,"user":{"displayName":"Ameck Dosseh","userId":"09603352587859208137"},"user_tz":-60},"id":"0iFb5gpzuZXD","outputId":"b0cfb56a-530d-4959-ee8d-0a1580158ed6","execution":{"iopub.status.busy":"2023-06-11T00:37:02.879626Z","iopub.execute_input":"2023-06-11T00:37:02.880001Z","iopub.status.idle":"2023-06-11T00:37:03.870054Z","shell.execute_reply.started":"2023-06-11T00:37:02.879962Z","shell.execute_reply":"2023-06-11T00:37:03.868927Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"DeepLabv3FineTuning\t\t     Train.csv\nSampleSubmission.csv\t\t     Train.zip\nSlidinVideos_StarterNotebook.ipynb   __notebook__.ipynb\nSubmission_fcn_resnet50.csv\t     __notebook_source__.ipynb\nSubmission_fcn_resnet50_epchs50.csv  __output__.json\nTest\t\t\t\t     __results__.html\nTest.zip\t\t\t     custom.css\n","output_type":"stream"}]},{"cell_type":"code","source":"#### Download title masks collection and unzip it to the cloned repository. \n\n# !unzip -q Train.zip -d DeepLabv3FineTuning/","metadata":{"executionInfo":{"elapsed":39050,"status":"ok","timestamp":1685804043920,"user":{"displayName":"Ameck Dosseh","userId":"09603352587859208137"},"user_tz":-60},"id":"1jkEZp1euL8c","execution":{"iopub.status.busy":"2023-06-11T00:37:03.873271Z","iopub.execute_input":"2023-06-11T00:37:03.873588Z","iopub.status.idle":"2023-06-11T00:37:03.880561Z","shell.execute_reply.started":"2023-06-11T00:37:03.873558Z","shell.execute_reply":"2023-06-11T00:37:03.879675Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"!ls DeepLabv3FineTuning","metadata":{"execution":{"iopub.status.busy":"2023-06-11T00:37:03.882007Z","iopub.execute_input":"2023-06-11T00:37:03.882344Z","iopub.status.idle":"2023-06-11T00:37:04.883274Z","shell.execute_reply.started":"2023-06-11T00:37:03.882309Z","shell.execute_reply":"2023-06-11T00:37:04.881997Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"Analysis.ipynb\tREADME.md\t\t       datahandler.py\t segdataset.py\nCFExp\t\tSegmentationDatasetDemo.ipynb  environment.yml\t trainer.py\nCrackForest\tTests\t\t\t       main.py\nDemoExp\t\tTrain\t\t\t       model.py\nLICENSE.md\t__pycache__\t\t       requirements.txt\n","output_type":"stream"}]},{"cell_type":"code","source":"# !mkdir DeepLabv3FineTuning/Train","metadata":{"execution":{"iopub.status.busy":"2023-06-11T00:37:04.886975Z","iopub.execute_input":"2023-06-11T00:37:04.887383Z","iopub.status.idle":"2023-06-11T00:37:04.892717Z","shell.execute_reply.started":"2023-06-11T00:37:04.887353Z","shell.execute_reply":"2023-06-11T00:37:04.891172Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"# !mv DeepLabv3FineTuning/Images DeepLabv3FineTuning/Train\n# !mv DeepLabv3FineTuning/Masks DeepLabv3FineTuning/Train","metadata":{"execution":{"iopub.status.busy":"2023-06-11T00:37:04.893834Z","iopub.execute_input":"2023-06-11T00:37:04.894656Z","iopub.status.idle":"2023-06-11T00:37:05.337691Z","shell.execute_reply.started":"2023-06-11T00:37:04.894623Z","shell.execute_reply":"2023-06-11T00:37:05.336656Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"#### Training DeepLab model: our goal is to maximize test_f1_score\n#### You may want to play with number of epochs, learning rate and loss function to get better results\n\n%cd DeepLabv3FineTuning\nfrom pathlib import Path\n\n#from segmentation_models_pytorch.losses.jaccard  import JaccardLoss\n#from segmentation_models_pytorch.losses.constants import BINARY_MODE\n#criterion = JaccardLoss(mode=BINARY_MODE)\n\nimport torch\nfrom sklearn.metrics import f1_score, roc_auc_score\nfrom torch.utils import data\n\nimport datahandler\nfrom model import createDeepLabv3\nfrom trainer import train_model","metadata":{"id":"TbZX1CnauSww","outputId":"ab8960ff-2e89-4432-bc07-a9f42e1ba291","execution":{"iopub.status.busy":"2023-06-11T00:37:05.343096Z","iopub.execute_input":"2023-06-11T00:37:05.343831Z","iopub.status.idle":"2023-06-11T00:37:05.650308Z","shell.execute_reply.started":"2023-06-11T00:37:05.343795Z","shell.execute_reply":"2023-06-11T00:37:05.649409Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"/kaggle/working/DeepLabv3FineTuning\n","output_type":"stream"}]},{"cell_type":"code","source":"\"\"\" FCN Model download and change the head for your prediction\"\"\"\nfrom torchvision.models.segmentation.fcn import FCNHead\nfrom torchvision.models.segmentation import FCN_ResNet101_Weights\nfrom torchvision import models\n\n\ndef createFCN(outputchannels=1):\n    \"\"\"FCN class with custom head\n\n    Args:\n        outputchannels (int, optional): The number of output channels\n        in your dataset masks. Defaults to 1.\n\n    Returns:\n        model: Returns the DeepLabv3 model with the ResNet50 backbone.\n    \"\"\"\n    model = models.segmentation.fcn_resnet101(pretrained=True,\n                                                    progress=True,\n                                             weights=FCN_ResNet101_Weights.DEFAULT\n                                             )\n    model.classifier = FCNHead(2048, outputchannels)\n    # Set the model in training mode\n    model.train()\n    return model","metadata":{"execution":{"iopub.status.busy":"2023-06-11T00:37:05.651660Z","iopub.execute_input":"2023-06-11T00:37:05.652039Z","iopub.status.idle":"2023-06-11T00:37:05.658666Z","shell.execute_reply.started":"2023-06-11T00:37:05.652006Z","shell.execute_reply":"2023-06-11T00:37:05.657798Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"# Create the deeplabv3 resnet101 model which is pretrained on a subset\n# of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset.\nmodel = createFCN()\nmodel.train()","metadata":{"id":"IE1qQYjOwGg3","execution":{"iopub.status.busy":"2023-06-11T00:37:05.659991Z","iopub.execute_input":"2023-06-11T00:37:05.660548Z","iopub.status.idle":"2023-06-11T00:37:07.904438Z","shell.execute_reply.started":"2023-06-11T00:37:05.660515Z","shell.execute_reply":"2023-06-11T00:37:07.903369Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/fcn_resnet101_coco-7ecb50ca.pth\" to /root/.cache/torch/hub/checkpoints/fcn_resnet101_coco-7ecb50ca.pth\n100%|██████████| 208M/208M [00:00<00:00, 246MB/s]  \n","output_type":"stream"},{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"FCN(\n  (backbone): IntermediateLayerGetter(\n    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (relu): ReLU(inplace=True)\n    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n    (layer1): Sequential(\n      (0): Bottleneck(\n        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (downsample): Sequential(\n          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): Bottleneck(\n        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (2): Bottleneck(\n        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n    )\n    (layer2): Sequential(\n      (0): Bottleneck(\n        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (downsample): Sequential(\n          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): Bottleneck(\n        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (2): Bottleneck(\n        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (3): Bottleneck(\n        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n    )\n    (layer3): Sequential(\n      (0): Bottleneck(\n        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (downsample): Sequential(\n          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (2): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (3): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (4): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (5): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (6): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (7): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (8): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (9): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (10): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (11): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (12): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (13): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (14): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (15): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (16): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (17): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (18): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (19): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (20): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (21): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (22): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n    )\n    (layer4): Sequential(\n      (0): Bottleneck(\n        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (downsample): Sequential(\n          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): Bottleneck(\n        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (2): Bottleneck(\n        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n    )\n  )\n  (classifier): FCNHead(\n    (0): Conv2d(2048, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU()\n    (3): Dropout(p=0.1, inplace=False)\n    (4): Conv2d(512, 1, kernel_size=(1, 1), stride=(1, 1))\n  )\n  (aux_classifier): FCNHead(\n    (0): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU()\n    (3): Dropout(p=0.1, inplace=False)\n    (4): Conv2d(256, 21, kernel_size=(1, 1), stride=(1, 1))\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"from pathlib import Path\n\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\n\nfrom segdataset import SegmentationDataset\nimport multiprocessing\npool = multiprocessing.Pool()\n\n\ndef get_dataloader_sep_folder(data_dir: str,\n                              image_folder: str = 'Image',\n                              mask_folder: str = 'Mask',\n                              batch_size: int = 4):\n    \"\"\" Create Train and Test dataloaders from two\n        separate Train and Test folders.\n        The directory structure should be as follows.\n        data_dir\n        --Train\n        ------Image\n        ---------Image1\n        ---------ImageN\n        ------Mask\n        ---------Mask1\n        ---------MaskN\n        --Test\n        ------Image\n        ---------Image1\n        ---------ImageM\n        ------Mask\n        ---------Mask1\n        ---------MaskM\n\n    Args:\n        data_dir (str): The data directory or root.\n        image_folder (str, optional): Image folder name. Defaults to 'Image'.\n        mask_folder (str, optional): Mask folder name. Defaults to 'Mask'.\n        batch_size (int, optional): Batch size of the dataloader. Defaults to 4.\n\n    Returns:\n        dataloaders: Returns dataloaders dictionary containing the\n        Train and Test dataloaders.\n    \"\"\"\n    data_transforms = transforms.Compose([transforms.ToTensor()])\n\n    image_datasets = {\n        x: SegmentationDataset(root=Path(data_dir) / x,\n                               transforms=data_transforms,\n                               image_folder=image_folder,\n                               mask_folder=mask_folder)\n        for x in ['Train', 'Test']\n    }\n    dataloaders = {\n        x: DataLoader(image_datasets[x],\n                      batch_size=batch_size,\n                      shuffle=True,\n                      num_workers=pool._processes)\n        for x in ['Train', 'Test']\n    }\n    return dataloaders\n\n\ndef get_dataloader_single_folder(data_dir: str,\n                                 image_folder: str = 'Images',\n                                 mask_folder: str = 'Masks',\n                                 fraction: float = 0.2,\n                                 batch_size: int = 4):\n    \"\"\"Create train and test dataloader from a single directory containing\n    the image and mask folders.\n\n    Args:\n        data_dir (str): Data directory path or root\n        image_folder (str, optional): Image folder name. Defaults to 'Images'.\n        mask_folder (str, optional): Mask folder name. Defaults to 'Masks'.\n        fraction (float, optional): Fraction of Test set. Defaults to 0.2.\n        batch_size (int, optional): Dataloader batch size. Defaults to 4.\n\n    Returns:\n        dataloaders: Returns dataloaders dictionary containing the\n        Train and Test dataloaders.\n    \"\"\"\n    data_transforms = transforms.Compose([transforms.ToTensor()])\n\n    image_datasets = {\n        x: SegmentationDataset(data_dir,\n                               image_folder=image_folder,\n                               mask_folder=mask_folder,\n                               seed=100,\n                               fraction=fraction,\n                               subset=x,\n                               transforms=data_transforms)\n        for x in ['Train', 'Test']\n    }\n    dataloaders = {\n        x: DataLoader(image_datasets[x],\n                      batch_size=batch_size,\n                      shuffle=True,\n                      num_workers=pool._processes)\n        for x in ['Train', 'Test']\n    }\n    return dataloaders","metadata":{"execution":{"iopub.status.busy":"2023-06-11T00:37:07.905891Z","iopub.execute_input":"2023-06-11T00:37:07.906797Z","iopub.status.idle":"2023-06-11T00:37:07.994950Z","shell.execute_reply.started":"2023-06-11T00:37:07.906762Z","shell.execute_reply":"2023-06-11T00:37:07.993314Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"datahandler.get_dataloader_single_folder = get_dataloader_single_folder","metadata":{"execution":{"iopub.status.busy":"2023-06-11T00:37:07.999408Z","iopub.execute_input":"2023-06-11T00:37:07.999738Z","iopub.status.idle":"2023-06-11T00:37:08.005209Z","shell.execute_reply.started":"2023-06-11T00:37:07.999704Z","shell.execute_reply":"2023-06-11T00:37:08.004315Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"data_directory = Path(\"Train\")\n# Create the experiment directory if not present\nexp_directory = Path(\"DemoExp\")\nif not exp_directory.exists():\n    exp_directory.mkdir()\n\nepochs = 50\n\n# Specify the evaluation metrics\nmetrics = {'f1_score': f1_score, 'auroc': roc_auc_score}\n\n# Create the dataloader\ndataloaders = datahandler.get_dataloader_single_folder(\n    data_directory, fraction=0.2, image_folder=\"Images\", \n    mask_folder=\"Masks\", batch_size=4)\n\n\n\n# Specify the loss function\ncriterion = torch.nn.MSELoss(reduction='mean')\n\n# Specify the optimizer with a lower learning rate\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n_ = train_model(model, \n                criterion,\n                dataloaders,\n                optimizer,\n                bpath=exp_directory,\n                metrics=metrics,\n                num_epochs=epochs)\n","metadata":{"id":"36mJmWfrwHiu","execution":{"iopub.status.busy":"2023-06-11T00:37:08.007146Z","iopub.execute_input":"2023-06-11T00:37:08.007924Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/50\n----------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 427/427 [12:15<00:00,  1.72s/it]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0155\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 107/107 [01:47<00:00,  1.01s/it]","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.0097\n{'epoch': 1, 'Train_loss': 0.01546681672334671, 'Test_loss': 0.009711787104606628, 'Train_f1_score': 0.48038826291974335, 'Train_auroc': 0.9557239986366792, 'Test_f1_score': 0.6213690589186102, 'Test_auroc': 0.9750829562369402}\nEpoch 2/50\n----------\n","output_type":"stream"},{"name":"stderr","text":"\n100%|██████████| 427/427 [12:08<00:00,  1.71s/it]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0131\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 107/107 [01:45<00:00,  1.01it/s]","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.0132\n{'epoch': 2, 'Train_loss': 0.013147267512977123, 'Test_loss': 0.013223299756646156, 'Train_f1_score': 0.6371784069544981, 'Train_auroc': 0.983394182124267, 'Test_f1_score': 0.7146741891498855, 'Test_auroc': 0.9776998853046144}\nEpoch 3/50\n----------\n","output_type":"stream"},{"name":"stderr","text":"\n 73%|███████▎  | 311/427 [08:54<03:16,  1.70s/it]","output_type":"stream"}]},{"cell_type":"code","source":"\n# Save the trained model\ntorch.save(model, exp_directory / 'weights.pt')\n\n# Terminate\nexit()\n%cd ../\n","metadata":{"id":"E0_cEwS_wP7P","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## 2. Making the submission\n#### Unpack the testset\n# !unzip -q Test.zip","metadata":{"id":"sba8VNgMwQ3f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !mkdir Test\n# !mv Images/ Test/","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#### Load finetuned deeplab model and define helper functions\n\nimport glob\nimport os\nimport json\nimport cv2    \nimport torch\nimport numpy as np\nimport pandas as pd\n\nfrom torchvision import transforms  \nfrom PIL import Image\n\ndeeplab_width = 768\ndeeplab_height = 432\ndeeplab = torch.load('DeepLabv3FineTuning/DemoExp/weights.pt')\ndeeplab.eval()\n     \n\ndef boxify(out, boxes):\n    return [ torch.mean(out[b[0]:b[1], b[2]:b[3]]).item()  for b in boxes ]      \n\ndef titlesegment(image, boxes):\n    # Resize the frame to the training input size\n    inputs = transforms.ToTensor()(Image.fromarray(image, mode=\"RGB\"))\n    inputs = inputs.reshape(1, *inputs.shape).to('cuda')\n    \n    with torch.set_grad_enabled(False):\n        outputs = deeplab(inputs)\n        out = outputs['out'][0][0]\n        box_scores = boxify(out, boxes)\n        return out, np.array(box_scores)\n    \ndef occupation(boxes):\n    area = lambda b: (b[1]-b[0]) * (b[3]-b[2])\n    y1, y2 = min([b[0] for b in boxes ]), max([b[1] for b in boxes ])\n    x1, x2 = min([b[2] for b in boxes ]), max([b[3] for b in boxes ])\n    return 1.0 * sum([ area(b) for b in boxes ]) / area([y1, y2, x1, x2])\n\n# Locates a paragraph of text with the highest probability \n# to be a slide title \ndef pickonetitle(boxes, scores):\n    if len(boxes) == 0:\n        return []\n        \n    best_idx = np.argmax(scores)\n    best = boxes[best_idx]\n\n    candidates = { i for i, box in enumerate(boxes) \n        if abs(box['fontsize']-best['fontsize']) <= best['fontsize'] * 0.25 \n            and scores[best_idx] - scores[i] < 0.6 }\n    \n    while occupation([ boxes[i]['bbox'] for i in candidates ]) < 0.75:\n        combinations = [ candidates - {j} for j in candidates if j != best_idx ]\n        candidates = max(combinations, \n            key=lambda comb: occupation([ boxes[j]['bbox'] for j in comb ]))\n            \n    title_boxes = [ boxes[i] for i in candidates ]\n    return title_boxes    \n\n#### Now, we're going to use EasyOCR package to \"read\" the titles. Modern OCR solutions may still produce a handful of errors, which is why you're encouraged to apply a language model or alternative OCR model to fix possible misspelings. \n\nfrom shapely.geometry import Polygon, LineString\n\nimport easyocr\nreader = easyocr.Reader(['en']) \n\ndef sameline(a, b):\n    return ((min(a['bbox'][1], b['bbox'][1]) - max(a['bbox'][0], b['bbox'][0]))\n            / min(a['height'], b['height'])) >= 0.8\n            \ndef isbelow(a, b):\n    return not sameline(a, b) and a['bbox'][0] > b['bbox'][0]  \n    \ndef isright(a, b):\n    return sameline(a, b) and a['bbox'][2] > b['bbox'][2]  \n\ndef sort_boxes(boxes):              \n    boxes_ = []\n    for b in boxes:\n        i = 0\n        while i < len(boxes_) and (isbelow(b, boxes_[i]) or isright(b, boxes_[i])):\n            i += 1\n        boxes_.insert(i, b)\n    return boxes_\n\ndef _norm_bbox(bbox):\n    return  [ int(max(0, min([ p[1] for p in bbox ]))), \n              int(max([ p[1] for p in bbox ])), \n              int(max(0, min([ p[0] for p in bbox ]))), \n              int(max([ p[0] for p in bbox ])) ]\n\ndef process_ocr(ocr_results):\n    items = []\n    for bbox, text, conf in ocr_results:\n        if text == '' or conf <= 0:\n            continue  \n        if not isinstance(bbox, list):\n            bbox = bbox.tolist()\n        item = {}\n        item['width'] = LineString(bbox[0:2]).length\n        item['height'] = LineString(bbox[1:3]).length\n        item['area'] = item['width'] * item['height']\n        item['fontsize'] = 1.0 * item['width'] / len(text)\n        item['bbox'] = _norm_bbox(bbox)            \n        item['text'] = text\n        item['conf'] = float(conf)\n        items.append(item)\n    return items     \n\ndef read_text(img):\n    return process_ocr(reader.readtext(img))\n\n### Data exploration\n#### First, we will pick some random test slide and will extract any instance of text on it\n\nfrom IPython.display import display, JSON, Image as IImage\n\ndef draw_boxes(frame, boxes):\n    for box in boxes:\n        frame = cv2.rectangle(frame, (box[2], box[0]), (box[3], box[1]), (36,255,12), 2)\n    return frame","metadata":{"id":"xGRe2fN0wVFN","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls Test/Images/*33.jpg","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls Test/Images/*.jpg | wc -l","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !ls Test/Images/*.jpg | head -n 5","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\n''.join(re.findall(r'\\d+', \"Id_0khgx7z8tk.jpg\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load some test image and read the text on it\nslide_ = cv2.imread('Test/Images/Id_76dbm25833.jpg')\n\ntextboxes_ = read_text(slide_)\n\ntextdata_ = pd.DataFrame(list(textboxes_))\ndel textdata_['conf']\n             \nslide_debug_ = draw_boxes(slide_, [ box['bbox'] for box in textboxes_ ])\n\ndisplay(f\"Test slide #33\", \n    Image.fromarray(slide_debug_).resize((640,360)), textdata_.head(5))","metadata":{"id":"Px7j8fmXwirJ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"slide_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#### Now, we will apply finetuned DeepLab model and \"score\" each textbox as to whether it might be a title or not\n\nout, scores = titlesegment(slide_, [ box['bbox'] for box in textboxes_ ])\nout_mask = (np.abs(out.cpu().numpy())*255).astype(np.uint8)\n\ntextdata_['score'] = scores\n\ndisplay(f\"Test slide #33 (segmentation output)\", Image.fromarray(out_mask).resize((640,360)), textdata_.head(5))\n","metadata":{"id":"LtmwXdD6wn80","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# [''.join(re.findall(r'\\d+', x)) for x in os.listdir('Test/Images')]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#### Iterate this method on all test images\nfrom tqdm import tqdm\n\nimages = sorted(glob.glob('Test/Images/*.jpg'))\nresult = []\n\nfor path in tqdm(images):\n    image = cv2.imread(path)\n    textboxes = read_text(image)\n\n    out, scores = titlesegment(image, [ box['bbox'] for box in textboxes ])\n    titleboxes = pickonetitle(textboxes, scores)\n    \n    # Sort text boxes as you would read it in English: from top to bottom\n    # and from left to right\n    titleboxes = sort_boxes(titleboxes)\n    title = ' '.join(box['text'] for box in titleboxes)    \n    \n    image_id = path.split(\"/\")[-1] #int(''.join(re.findall(r'\\d+', path.split('.')[0].split('/')[-1])))\n    result.append([image_id, title])\n\n\n#### Convert the result to a dataframe and save it\n\ndf = pd.DataFrame.from_records(result, columns=['ID','Title'])\ndf\n","metadata":{"id":"IyHd80jFwrWi","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.to_csv('Submission_fcn_resnet101_epchs50.csv', index=False)","metadata":{"executionInfo":{"elapsed":626,"status":"ok","timestamp":1685803454703,"user":{"displayName":"Ameck Dosseh","userId":"09603352587859208137"},"user_tz":-60},"id":"GedXAwZ0tog7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}